{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unique-malawi",
   "metadata": {},
   "source": [
    "## Gradient Calculation with Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "robust-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "included-veteran",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3636, 0.5524, 0.1028], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, requires_grad=True) #specify grad function and \n",
    "                                    # create cakobian matrix (chain rule)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "atmospheric-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3636, 2.5524, 2.1028], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2 # creating a node, y will have attribute grad_fn\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "saved-glasgow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.7273, 5.1048, 4.2055], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * 2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "informative-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6792, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = z.mean() # get mean and z becomes scalar\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "colored-effectiveness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6667, 0.6667, 0.6667])\n"
     ]
    }
   ],
   "source": [
    "z.backward() # dz/dx - calculate gradient z with respect to x\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "infectious-belle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8667, 0.8667, 0.6867])\n"
     ]
    }
   ],
   "source": [
    "# if not scalar a vector is needed to be passed into backward fun\n",
    "# because in the background is is a sacordian matrix\n",
    "z2 = y * 2\n",
    "v = torch.tensor([0.1,0.1,0.01], dtype=torch.float32)\n",
    "z2.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "naked-colony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5750, 0.8646, 0.0301], requires_grad=True)\n",
      "tensor([0.5750, 0.8646, 0.0301])\n"
     ]
    }
   ],
   "source": [
    "#how to prevent pytorch to track gradients\n",
    "x = torch.rand(3, requires_grad=True)\n",
    "print(x)\n",
    "# 3 options\n",
    "# First Option\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "spoken-protection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0970, 0.9302, 0.5458], requires_grad=True)\n",
      "tensor([0.0970, 0.9302, 0.5458])\n"
     ]
    }
   ],
   "source": [
    "# Second Option\n",
    "x2 = torch.rand(3, requires_grad=True)\n",
    "print(x2)\n",
    "y = x2.detach() # use detach\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "respective-preservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2811, 0.5240, 0.5349], requires_grad=True)\n",
      "tensor([2.2811, 2.5240, 2.5349])\n"
     ]
    }
   ],
   "source": [
    "# Third Option\n",
    "x3 = torch.rand(3, requires_grad=True)\n",
    "print(x3)\n",
    "with torch.no_grad():\n",
    "    y = x3 + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "average-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    weights.grad.zero_() # empty the gradient or we can use optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "republican-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b64372870b2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Python3.6\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Python3.6\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             raise TypeError(\"params argument given to the optimizer should be \"\n\u001b[0m\u001b[0;32m     42\u001b[0m                             \u001b[1;34m\"an iterable of Tensors or dicts, but got \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                             torch.typename(params))\n",
      "\u001b[1;31mTypeError\u001b[0m: params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(5, requires_grad=True)\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-giant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myai_env",
   "language": "python",
   "name": "myai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
